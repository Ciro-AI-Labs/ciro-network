//! AI Job Execution Engine
//!
//! This module handles the execution of AI jobs using the model registry and framework manager.
//! It provides job scheduling, resource management, and result handling.

use std::collections::HashMap;
use std::path::Path;
use std::process::{Command, Stdio};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;
use tokio::time::timeout;
use serde::{Deserialize, Serialize};
use anyhow::{Result, anyhow};
use tracing::{info, error, debug};

use crate::ai::model_registry::{ModelRegistry, ModelInfo};
use crate::ai::frameworks::{FrameworkManager, ExecutionContext};
use crate::node::coordinator::{JobType, CVTaskType, NLPTaskType, AudioTaskType, TimeSeriesTaskType, MultimodalTaskType, RLTaskType, AIDomain};
use crate::types::{JobId, TaskId, WorkerId};

/// AI job execution request
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AIJobRequest {
    pub job_id: JobId,
    pub task_id: TaskId,
    pub worker_id: WorkerId,
    pub job_type: JobType,
    pub input_data: AIJobInput,
    pub output_requirements: AIJobOutput,
    pub resource_constraints: ResourceConstraints,
    pub execution_params: ExecutionParams,
}

/// AI job input data
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AIJobInput {
    pub data_type: String,
    pub data_path: String,
    pub metadata: HashMap<String, serde_json::Value>,
    pub preprocessing_required: bool,
}

/// AI job output requirements
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AIJobOutput {
    pub format: String,
    pub path: String,
    pub postprocessing_required: bool,
    pub quality_requirements: Option<QualityRequirements>,
}

/// Quality requirements for AI outputs
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QualityRequirements {
    pub min_accuracy: Option<f32>,
    pub max_latency_ms: Option<u32>,
    pub max_memory_mb: Option<u32>,
    pub confidence_threshold: Option<f32>,
}

/// Resource constraints for execution
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ResourceConstraints {
    pub max_gpu_memory_gb: u32,
    pub max_cpu_cores: u32,
    pub max_ram_gb: u32,
    pub max_execution_time_seconds: u32,
    pub gpu_required: bool,
    pub specialized_hardware: Option<String>,
}

/// Execution parameters
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionParams {
    pub batch_size: u32,
    pub precision: String, // "fp32", "fp16", "int8"
    pub optimization_level: String, // "speed", "balanced", "accuracy"
    pub cache_enabled: bool,
    pub additional_params: HashMap<String, String>,
}

/// AI job execution result
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AIJobResult {
    pub job_id: JobId,
    pub task_id: TaskId,
    pub worker_id: WorkerId,
    pub status: ExecutionStatus,
    pub output_data: Option<AIJobOutput>,
    pub performance_metrics: PerformanceMetrics,
    pub error_details: Option<String>,
    pub execution_log: Vec<String>,
}

/// Execution status
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub enum ExecutionStatus {
    Pending,
    Running,
    Completed,
    Failed,
    Cancelled,
    Timeout,
}

/// Performance metrics for executed jobs
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceMetrics {
    pub execution_time_ms: u64,
    pub cpu_usage_percent: f32,
    pub memory_usage_mb: u32,
    pub gpu_usage_percent: Option<f32>,
    pub gpu_memory_usage_mb: Option<u32>,
    pub throughput_items_per_second: Option<f32>,
    pub accuracy: Option<f32>,
    pub confidence_score: Option<f32>,
}

/// AI job execution engine
pub struct AIExecutionEngine {
    model_registry: Arc<ModelRegistry>,
    framework_manager: Arc<FrameworkManager>,
    active_jobs: Arc<RwLock<HashMap<JobId, AIJobResult>>>,
    job_queue: Arc<RwLock<Vec<AIJobRequest>>>,
    max_concurrent_jobs: usize,
}

impl AIExecutionEngine {
    /// Create a new AI execution engine
    pub fn new(
        model_registry: Arc<ModelRegistry>,
        framework_manager: Arc<FrameworkManager>,
        max_concurrent_jobs: usize,
    ) -> Self {
        Self {
            model_registry,
            framework_manager,
            active_jobs: Arc::new(RwLock::new(HashMap::new())),
            job_queue: Arc::new(RwLock::new(Vec::new())),
            max_concurrent_jobs,
        }
    }

    /// Submit an AI job for execution
    pub async fn submit_job(&self, request: AIJobRequest) -> Result<()> {
        info!("Submitting AI job {} for execution", request.job_id);
        
        // Validate the job request
        self.validate_job_request(&request).await?;
        
        // Add to job queue
        self.job_queue.write().await.push(request);
        
        // Try to process jobs immediately
        self.process_job_queue().await?;
        
        Ok(())
    }

    /// Get job execution status
    pub async fn get_job_status(&self, job_id: &JobId) -> Option<AIJobResult> {
        self.active_jobs.read().await.get(job_id).cloned()
    }

    /// Cancel a running job
    pub async fn cancel_job(&self, job_id: &JobId) -> Result<()> {
        let mut active_jobs = self.active_jobs.write().await;
        
        if let Some(job_result) = active_jobs.get_mut(job_id) {
            if job_result.status == ExecutionStatus::Running {
                job_result.status = ExecutionStatus::Cancelled;
                info!("Cancelled job {}", job_id);
                return Ok(());
            }
        }
        
        Err(anyhow!("Job {} not found or not running", job_id))
    }

    /// Process the job queue
    async fn process_job_queue(&self) -> Result<()> {
        let active_count = self.active_jobs.read().await.len();
        
        if active_count >= self.max_concurrent_jobs {
            debug!("Maximum concurrent jobs reached, queuing job");
            return Ok(());
        }
        
        let mut job_queue = self.job_queue.write().await;
        
        while let Some(request) = job_queue.pop() {
            if self.active_jobs.read().await.len() >= self.max_concurrent_jobs {
                // Put the job back and break
                job_queue.push(request);
                break;
            }
            
            // Start job execution
            self.execute_job_async(request).await?;
        }
        
        Ok(())
    }

    /// Execute a job asynchronously
    async fn execute_job_async(&self, request: AIJobRequest) -> Result<()> {
        let job_id = request.job_id;
        
        // Create initial job result
        let job_result = AIJobResult {
            job_id,
            task_id: request.task_id,
            worker_id: request.worker_id,
            status: ExecutionStatus::Pending,
            output_data: None,
            performance_metrics: PerformanceMetrics {
                execution_time_ms: 0,
                cpu_usage_percent: 0.0,
                memory_usage_mb: 0,
                gpu_usage_percent: None,
                gpu_memory_usage_mb: None,
                throughput_items_per_second: None,
                accuracy: None,
                confidence_score: None,
            },
            error_details: None,
            execution_log: Vec::new(),
        };
        
        // Add to active jobs
        self.active_jobs.write().await.insert(job_id, job_result);
        
        // Clone necessary data for the async task
        let model_registry = Arc::clone(&self.model_registry);
        let framework_manager = Arc::clone(&self.framework_manager);
        let active_jobs = Arc::clone(&self.active_jobs);
        
        // Spawn the execution task
        tokio::spawn(async move {
            let result = Self::execute_job_internal(
                model_registry,
                framework_manager,
                request,
            ).await;
            
            // Update job result
            let mut active_jobs = active_jobs.write().await;
            if let Some(job_result) = active_jobs.get_mut(&job_id) {
                match result {
                    Ok(metrics) => {
                        job_result.status = ExecutionStatus::Completed;
                        job_result.performance_metrics = metrics;
                        info!("Job {} completed successfully", job_id);
                    }
                    Err(e) => {
                        job_result.status = ExecutionStatus::Failed;
                        job_result.error_details = Some(e.to_string());
                        error!("Job {} failed: {}", job_id, e);
                    }
                }
            }
        });
        
        Ok(())
    }

    /// Internal job execution logic
    async fn execute_job_internal(
        model_registry: Arc<ModelRegistry>,
        framework_manager: Arc<FrameworkManager>,
        request: AIJobRequest,
    ) -> Result<PerformanceMetrics> {
        let start_time = Instant::now();
        
        // Find the best model for this job
        let model = Self::select_model_for_job(&model_registry, &request)?;
        
        // Create execution context
        let context = ExecutionContext::new(
            model.framework.clone(),
            model.name.clone(),
            request.input_data.data_path.clone(),
            request.output_requirements.path.clone(),
        )
        .with_gpu(request.resource_constraints.gpu_required)
        .with_batch_size(request.execution_params.batch_size)
        .with_timeout(request.resource_constraints.max_execution_time_seconds);
        
        // Generate Docker command
        let docker_command = framework_manager.generate_docker_command(
            &model,
            &request.input_data.data_path,
            &request.output_requirements.path,
            request.resource_constraints.gpu_required,
        )?;
        
        // Execute the job
        let execution_result = Self::execute_docker_command(
            docker_command,
            request.resource_constraints.max_execution_time_seconds,
        ).await?;
        
        let execution_time = start_time.elapsed();
        
        // Create performance metrics
        let metrics = PerformanceMetrics {
            execution_time_ms: execution_time.as_millis() as u64,
            cpu_usage_percent: execution_result.cpu_usage,
            memory_usage_mb: execution_result.memory_usage,
            gpu_usage_percent: execution_result.gpu_usage,
            gpu_memory_usage_mb: execution_result.gpu_memory_usage,
            throughput_items_per_second: execution_result.throughput,
            accuracy: execution_result.accuracy,
            confidence_score: execution_result.confidence,
        };
        
        Ok(metrics)
    }

    /// Select the best model for a job
    fn select_model_for_job(
        model_registry: &ModelRegistry,
        request: &AIJobRequest,
    ) -> Result<ModelInfo> {
        let task_name = Self::job_type_to_task_name(&request.job_type);
        
        let best_model = model_registry.find_best_model(
            &task_name,
            request.resource_constraints.max_gpu_memory_gb,
            request.resource_constraints.max_cpu_cores,
            request.resource_constraints.max_ram_gb,
        );
        
        best_model
            .cloned()
            .ok_or_else(|| anyhow!("No suitable model found for job type: {:?}", request.job_type))
    }

    /// Convert job type to task name for model selection
    fn job_type_to_task_name(job_type: &JobType) -> String {
        match job_type {
            JobType::ComputerVision { task_type, .. } => {
                match task_type {
                    CVTaskType::ObjectDetection => "object_detection".to_string(),
                    CVTaskType::ImageClassification => "image_classification".to_string(),
                    CVTaskType::ImageSegmentation => "image_segmentation".to_string(),
                    CVTaskType::FaceRecognition => "face_recognition".to_string(),
                    CVTaskType::FaceDetection => "face_detection".to_string(),
                    CVTaskType::OCR => "ocr".to_string(),
                    CVTaskType::ImageGeneration => "image_generation".to_string(),
                    CVTaskType::StyleTransfer => "style_transfer".to_string(),
                    CVTaskType::SuperResolution => "super_resolution".to_string(),
                    CVTaskType::ImageCaptioning => "image_captioning".to_string(),
                    CVTaskType::VisualQuestionAnswering => "visual_question_answering".to_string(),
                    CVTaskType::SceneUnderstanding => "scene_understanding".to_string(),
                    CVTaskType::DepthEstimation => "depth_estimation".to_string(),
                    CVTaskType::PoseEstimation => "pose_estimation".to_string(),
                    CVTaskType::Custom(name) => name.clone(),
                }
            }
            JobType::NLP { task_type, .. } => {
                match task_type {
                    NLPTaskType::SentimentAnalysis => "sentiment_analysis".to_string(),
                    NLPTaskType::TextClassification => "text_classification".to_string(),
                    NLPTaskType::NamedEntityRecognition => "named_entity_recognition".to_string(),
                    NLPTaskType::TextSummarization => "text_summarization".to_string(),
                    NLPTaskType::QuestionAnswering => "question_answering".to_string(),
                    NLPTaskType::Translation => "translation".to_string(),
                    NLPTaskType::TextGeneration => "text_generation".to_string(),
                    NLPTaskType::EmbeddingsGeneration => "embeddings_generation".to_string(),
                    NLPTaskType::CodeGeneration => "code_generation".to_string(),
                    NLPTaskType::CodeCompletion => "code_completion".to_string(),
                    NLPTaskType::ConversationalAI => "conversational_ai".to_string(),
                    NLPTaskType::TextToSpeech => "text_to_speech".to_string(),
                    NLPTaskType::LanguageModeling => "language_modeling".to_string(),
                    NLPTaskType::TokenClassification => "token_classification".to_string(),
                    NLPTaskType::Custom(name) => name.clone(),
                }
            }
            JobType::AudioProcessing { task_type, .. } => {
                match task_type {
                    AudioTaskType::SpeechToText => "speech_to_text".to_string(),
                    AudioTaskType::TextToSpeech => "text_to_speech".to_string(),
                    AudioTaskType::AudioClassification => "audio_classification".to_string(),
                    AudioTaskType::MusicGeneration => "music_generation".to_string(),
                    AudioTaskType::AudioEnhancement => "audio_enhancement".to_string(),
                    AudioTaskType::NoiseReduction => "noise_reduction".to_string(),
                    AudioTaskType::SpeakerIdentification => "speaker_identification".to_string(),
                    AudioTaskType::AudioTranscription => "audio_transcription".to_string(),
                    AudioTaskType::MusicInformationRetrieval => "music_information_retrieval".to_string(),
                    AudioTaskType::AudioSeparation => "audio_separation".to_string(),
                    AudioTaskType::VoiceConversion => "voice_conversion".to_string(),
                    AudioTaskType::Custom(name) => name.clone(),
                }
            }
            JobType::TimeSeriesAnalysis { task_type, .. } => {
                match task_type {
                    TimeSeriesTaskType::Forecasting => "forecasting".to_string(),
                    TimeSeriesTaskType::AnomalyDetection => "anomaly_detection".to_string(),
                    TimeSeriesTaskType::TrendAnalysis => "trend_analysis".to_string(),
                    TimeSeriesTaskType::SeasonalDecomposition => "seasonal_decomposition".to_string(),
                    TimeSeriesTaskType::ChangePointDetection => "change_point_detection".to_string(),
                    TimeSeriesTaskType::Clustering => "clustering".to_string(),
                    TimeSeriesTaskType::Classification => "classification".to_string(),
                    TimeSeriesTaskType::Regression => "regression".to_string(),
                    TimeSeriesTaskType::Custom(name) => name.clone(),
                }
            }
            JobType::MultimodalAI { task_type, .. } => {
                match task_type {
                    MultimodalTaskType::ImageCaptioning => "image_captioning".to_string(),
                    MultimodalTaskType::VisualQuestionAnswering => "visual_question_answering".to_string(),
                    MultimodalTaskType::VideoUnderstanding => "video_understanding".to_string(),
                    MultimodalTaskType::CrossModalRetrieval => "cross_modal_retrieval".to_string(),
                    MultimodalTaskType::MultimodalEmbeddings => "multimodal_embeddings".to_string(),
                    MultimodalTaskType::AudioVisualSpeechRecognition => "audio_visual_speech_recognition".to_string(),
                    MultimodalTaskType::VideoSummarization => "video_summarization".to_string(),
                    MultimodalTaskType::MultimodalSentimentAnalysis => "multimodal_sentiment_analysis".to_string(),
                    MultimodalTaskType::Custom(name) => name.clone(),
                }
            }
            JobType::ReinforcementLearning { task_type, .. } => {
                match task_type {
                    RLTaskType::PolicyOptimization => "policy_optimization".to_string(),
                    RLTaskType::ValueFunctionApproximation => "value_function_approximation".to_string(),
                    RLTaskType::ModelBasedRL => "model_based_rl".to_string(),
                    RLTaskType::ModelFreeRL => "model_free_rl".to_string(),
                    RLTaskType::MultiAgentRL => "multi_agent_rl".to_string(),
                    RLTaskType::HierarchicalRL => "hierarchical_rl".to_string(),
                    RLTaskType::InverseRL => "inverse_rl".to_string(),
                    RLTaskType::ImitationLearning => "imitation_learning".to_string(),
                    RLTaskType::Custom(name) => name.clone(),
                }
            }
            JobType::SpecializedAI { domain, task_type, .. } => {
                let domain_prefix = match domain {
                    AIDomain::Medical => "medical",
                    AIDomain::Scientific => "scientific",
                    AIDomain::Robotics => "robotics",
                    AIDomain::AutonomousSystems => "autonomous",
                    AIDomain::ClimateModeling => "climate",
                    AIDomain::Bioinformatics => "bioinformatics",
                    AIDomain::DrugDiscovery => "drug_discovery",
                    AIDomain::MaterialsScience => "materials",
                    AIDomain::Astronomy => "astronomy",
                    AIDomain::Finance => "finance",
                    AIDomain::Cybersecurity => "cybersecurity",
                    AIDomain::Custom(name) => name,
                };
                format!("{}_{}", domain_prefix, task_type)
            }
            _ => "general_ai".to_string(),
        }
    }

    /// Execute Docker command with timeout and monitoring
    async fn execute_docker_command(
        command: Vec<String>,
        timeout_seconds: u32,
    ) -> Result<ExecutionResult> {
        let timeout_duration = Duration::from_secs(timeout_seconds as u64);
        
        let result = timeout(timeout_duration, async {
            let mut cmd = Command::new(&command[0]);
            cmd.args(&command[1..])
                .stdout(Stdio::piped())
                .stderr(Stdio::piped());
            
            let output = cmd.output()?;
            
            // Parse output for metrics (this would be framework-specific)
            let execution_result = ExecutionResult {
                success: output.status.success(),
                stdout: String::from_utf8_lossy(&output.stdout).to_string(),
                stderr: String::from_utf8_lossy(&output.stderr).to_string(),
                cpu_usage: 0.0, // TODO: Implement actual monitoring
                memory_usage: 0,
                gpu_usage: None,
                gpu_memory_usage: None,
                throughput: None,
                accuracy: None,
                confidence: None,
            };
            
            Ok::<ExecutionResult, anyhow::Error>(execution_result)
        }).await;
        
        match result {
            Ok(Ok(exec_result)) => Ok(exec_result),
            Ok(Err(e)) => Err(e),
            Err(_) => Err(anyhow!("Job execution timed out after {} seconds", timeout_seconds)),
        }
    }

    /// Validate job request
    async fn validate_job_request(&self, request: &AIJobRequest) -> Result<()> {
        // Check if input data exists
        if !Path::new(&request.input_data.data_path).exists() {
            return Err(anyhow!("Input data path does not exist: {}", request.input_data.data_path));
        }
        
        // Check if output directory exists or can be created
        if let Some(parent) = Path::new(&request.output_requirements.path).parent() {
            if !parent.exists() {
                std::fs::create_dir_all(parent)?;
            }
        }
        
        // Validate resource constraints
        if request.resource_constraints.max_gpu_memory_gb == 0 && request.resource_constraints.gpu_required {
            return Err(anyhow!("GPU required but no GPU memory allocated"));
        }
        
        Ok(())
    }
}

/// Docker execution result
#[derive(Debug)]
struct ExecutionResult {
    success: bool,
    stdout: String,
    stderr: String,
    cpu_usage: f32,
    memory_usage: u32,
    gpu_usage: Option<f32>,
    gpu_memory_usage: Option<u32>,
    throughput: Option<f32>,
    accuracy: Option<f32>,
    confidence: Option<f32>,
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::ai::model_registry::ModelRegistry;
    use crate::ai::frameworks::FrameworkManager;

    #[tokio::test]
    async fn test_ai_execution_engine_creation() {
        let model_registry = Arc::new(ModelRegistry::new());
        let framework_manager = Arc::new(FrameworkManager::new());
        let engine = AIExecutionEngine::new(model_registry, framework_manager, 4);
        
        assert_eq!(engine.max_concurrent_jobs, 4);
    }

    #[test]
    fn test_job_type_to_task_name() {
        let job_type = JobType::ComputerVision {
            task_type: CVTaskType::ObjectDetection,
            model_name: "yolo".to_string(),
            input_images: vec![],
            output_format: "json".to_string(),
            confidence_threshold: 0.5,
            batch_size: 1,
            additional_params: HashMap::new(),
        };
        
        let task_name = AIExecutionEngine::job_type_to_task_name(&job_type);
        assert_eq!(task_name, "object_detection");
    }
} 